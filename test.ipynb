{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qdrant Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset('BeIR/scifact', 'corpus', split='corpus')\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Embedding\n",
    "\n",
    "`Trong phần này, chúng ta sẽ chọn thư viện FastEmbedded với các mô hình embedding được huấn luyện từ trước.\n",
    "Do sử dụng ONNX, nên các mô hình này có thể chạy một cách hiệu quả ngay cả trên CPU. Mô hình MiniLM-L6-v2 là mô hình nhẹ và phù hợp để bắt đầu làm quen với Qdrant`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 1670.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from fastembed.embedding import TextEmbedding\n",
    "\n",
    "dense_embedding_model = TextEmbedding('sentence-transformers/all-MiniLM-L6-v2')\n",
    "dense_embeddings = list(dense_embedding_model.passage_embed(dataset['text'][0:1]))\n",
    "print(len(dense_embeddings))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 29 files: 100%|██████████| 29/29 [00:00<00:00, 29071.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from fastembed.sparse.bm25 import Bm25\n",
    "\n",
    "bm25_embedding_model = Bm25('Qdrant/bm25')\n",
    "bm25_embeddings = list(bm25_embedding_model.passage_embed(dataset['text'][0:1]))\n",
    "print(len(bm25_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Late Interactionn Embeddings\n",
    "\n",
    "`Một cách tiếp cận mới là mô hình Late Interaction hay gọi là tương tác muộn, thay vì embedding các token và sau đó tổng hợp các vector embedding của các token để tạo thành vector embedding của một câu thì cơ chế tương tác muộn sẽ giữ lại các vector embedding của các token và sau đó tính toán độ tương đồng giữa các token của câu query và token của tài liệu. Tham số đánh giá được sử dụng trong cơ chế này là MAXSIM. Tham số này sẽ được sử dụng để đánh giá độ tương đồng giữa câu truy vấn và tài liệu `\n",
    "\n",
    "`Một mô hình sử dụng cơ chế tương tác muộn này là \"ColBert\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 1110.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from fastembed.late_interaction import LateInteractionTextEmbedding\n",
    "\n",
    "late_interaction_embedding_model = LateInteractionTextEmbedding('colbert-ir/colbertv2.0')\n",
    "late_interaction_embeddings = list(late_interaction_embedding_model.passage_embed(dataset['text'][0:1]))\n",
    "print(len(late_interaction_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting Data to Qdrant Collection\n",
    "\n",
    "`Tất cả các vector hiện có được đưa vào Qdrant Collectionn. Việc giữ các vector này trong một bộ duy nhất cho phép kết hợp các loại embedding khác nhau và thậm chí tạo ra một pipeline phức tạp nhiều bước.`\n",
    "\n",
    "`Để thực hiện chạy Qdrant, mở terminal và gõ lệnh sau đây: docker run -d -p 6333:6333 -p 6334:6334 qdrant/qdrant:v1.10.0`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "\n",
    "client = QdrantClient(\n",
    "    url='https://618dc965-c6d2-4c3b-93bd-26050204f910.us-east4-0.gcp.cloud.qdrant.io:6333',\n",
    "    api_key= 'AVW6PnZn-3ld6bw33exaLaohJd8JymLDne4Mk4nIE298dhtB_XS5hQ'\n",
    ")\n",
    "client.create_collection(\n",
    "    \"scifact1\",\n",
    "    vectors_config={\n",
    "        \"all-MiniLM-L6-v2\": models.VectorParams(\n",
    "            size=len(dense_embeddings[0]),\n",
    "            distance=models.Distance.COSINE,\n",
    "        ),\n",
    "        \"colbertv2.0\": models.VectorParams(\n",
    "            size=len(late_interaction_embeddings[0][0]),\n",
    "            distance=models.Distance.COSINE,\n",
    "            multivector_config=models.MultiVectorConfig(\n",
    "                comparator=models.MultiVectorComparator.MAX_SIM,\n",
    "            )\n",
    "        ),\n",
    "    },\n",
    "    sparse_vectors_config={\n",
    "        \"bm25\": models.SparseVectorParams(\n",
    "            modifier=models.Modifier.IDF,\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "batch_size = 4\n",
    "for batch in tqdm.tqdm(dataset.iter(batch_size=batch_size), \n",
    "                       total=len(dataset) // batch_size):\n",
    "    dense_embeddings = list(dense_embedding_model.passage_embed(batch[\"text\"]))\n",
    "    bm25_embeddings = list(bm25_embedding_model.passage_embed(batch[\"text\"]))\n",
    "    late_interaction_embeddings = list(late_interaction_embedding_model.passage_embed(batch[\"text\"]))\n",
    "    \n",
    "    client.upload_points(\n",
    "        \"scifact1\",\n",
    "        points=[\n",
    "            models.PointStruct(\n",
    "                id=int(batch[\"_id\"][i]),\n",
    "                vector={\n",
    "                    \"all-MiniLM-L6-v2\": dense_embeddings[i].tolist(),\n",
    "                    \"bm25\": bm25_embeddings[i].as_object(),\n",
    "                    \"colbertv2.0\": late_interaction_embeddings[i].tolist(),\n",
    "                },\n",
    "                payload={\n",
    "                    \"_id\": batch[\"_id\"][i],\n",
    "                    \"title\": batch[\"title\"][i],\n",
    "                    \"text\": batch[\"text\"][i],\n",
    "                }\n",
    "            )\n",
    "            for i, _ in enumerate(batch[\"_id\"])\n",
    "        ],\n",
    "        # We send a lot of embeddings at once, so it's best to reduce the batch size.\n",
    "        # Otherwise, we would have gigantic requests sent for each batch and we can\n",
    "        # easily reach the maximum size of a single request.\n",
    "        batch_size=batch_size,  \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "\n",
    "client = QdrantClient(\n",
    "    url='https://618dc965-c6d2-4c3b-93bd-26050204f910.us-east4-0.gcp.cloud.qdrant.io:6333',\n",
    "    api_key= 'AVW6PnZn-3ld6bw33exaLaohJd8JymLDne4Mk4nIE298dhtB_XS5hQ'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountResult(count=5183)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.count('scifact3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\RAG_Chatbot_3.12\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32m2024-11-04 22:19:28.299\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mfastembed.embedding\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[33m\u001b[1mDefaultEmbedding, FlagEmbedding, JinaEmbedding are deprecated.Use from fastembed import TextEmbedding instead.\u001b[0m\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<?, ?it/s]\n",
      "Fetching 29 files: 100%|██████████| 29/29 [00:00<?, ?it/s]\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 5018.31it/s]\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "from fastembed.embedding import TextEmbedding\n",
    "from fastembed.sparse.bm25 import Bm25\n",
    "from fastembed.late_interaction import LateInteractionTextEmbedding\n",
    "from config import load_config\n",
    "\n",
    "\n",
    "\n",
    "CONFIG = load_config()\n",
    "\n",
    "dense_embedding_model = TextEmbedding(CONFIG['qdrant']['dense_embedding'])\n",
    "sparse_embedding_model = Bm25(CONFIG['qdrant']['sparse_embedding'])\n",
    "late_interaction_embedding_model = LateInteractionTextEmbedding(CONFIG['qdrant']['late_interaction_embedding'])\n",
    "\n",
    "\n",
    "\n",
    "class HybridSearch:\n",
    "    def __init__(self):\n",
    "        self.client = QdrantClient(\n",
    "            url = CONFIG['qdrant']['url'],\n",
    "            api_key=CONFIG['qdrant']['api_key']\n",
    "        )\n",
    "    def query_docs(self, query_text):\n",
    "        dense_embedding_query = next(dense_embedding_model.query_embed(query_text))\n",
    "        sparse_embedding_query = next(sparse_embedding_model.query_embed(query_text))\n",
    "        late_interaction_embedding_query = next(late_interaction_embedding_model.query_embed(query_text))\n",
    "        prefetch = [\n",
    "            models.Prefetch(\n",
    "                query=dense_embedding_query,\n",
    "                using = 'all-MiniLM-L6-v2',\n",
    "                limit = 20\n",
    "            ),\n",
    "            models.Prefetch(\n",
    "                query=models.SparseVector(**sparse_embedding_query.as_object()),\n",
    "                using = 'bm25',\n",
    "                limit = 20\n",
    "            ),\n",
    "            models.Prefetch(\n",
    "                query = late_interaction_embedding_query,\n",
    "                using = 'colbertv2.0',\n",
    "                limit = 20\n",
    "            )\n",
    "        ]\n",
    "        print('DONE')\n",
    "        responses = self.client.query_points(\n",
    "            collection_name=CONFIG['qdrant']['collection_name'],\n",
    "            prefetch=prefetch,\n",
    "            query = models.FusionQuery(\n",
    "                fusion=models.Fusion.RRF\n",
    "            ),\n",
    "            with_payload=True,\n",
    "            limit = 10\n",
    "        )\n",
    "        score_threshold = CONFIG['qdrant']['thresh_score']\n",
    "        docs = ''\n",
    "        for i, response in enumerate(responses.points):\n",
    "            if response.score >= score_threshold:\n",
    "                docs += f'Example{i+1}:\\nTitle:{response.payload['title']}\\nParagraph: {response.payload['text']}\\n'\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = HybridSearch()\n",
    "query = \"What is the impact of COVID-19 on the environment?\"\n",
    "docs = search.query_docs(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
